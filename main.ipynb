{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the serial for arduino\n",
    "arduino = serial.Serial(port='/dev/cu.usbserial-0001',\n",
    "                        baudrate=9600, timeout=1)\n",
    "\n",
    "# Function to send data\n",
    "\n",
    "\n",
    "def send_data(data):\n",
    "    if arduino.isOpen():\n",
    "        arduino.write(data.encode())  # Send data as bytes\n",
    "        print(f\"Sent: {data}\")\n",
    "    else:\n",
    "        print(\"Serial port is not open!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Import time module for tracking intervals\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Get screen center\n",
    "ret, frame = cap.read()\n",
    "height, width, _ = frame.shape\n",
    "center_screen = (width // 2, height // 2)\n",
    "\n",
    "# Define class names and assign colors\n",
    "class_names = ['customer', 'talking', 'eye contact']\n",
    "colors = {\n",
    "    'customer': (0, 255, 255),  # Cyan for regular faces\n",
    "    'talking': (0, 255, 0),     # Green\n",
    "    'eye contact': (0, 0, 255)  # Red\n",
    "}\n",
    "target_color = (255, 0, 0)  # Red for the target face\n",
    "\n",
    "# Time tracking for 3-second interval\n",
    "last_sent_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    # frame = cv2.flip(frame, 1)\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform detection\n",
    "    results = model.predict(source=frame, conf=0.5)\n",
    "\n",
    "    closest_face = None\n",
    "    min_distance = float('inf')\n",
    "    closest_face_center = None  # To store center coordinates of the closest face\n",
    "\n",
    "    # Draw detections on the frame\n",
    "    for box in results[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box coordinates\n",
    "        conf = box.conf[0]  # Confidence score\n",
    "        label = int(box.cls)  # Class label as integer\n",
    "        # Retrieve class name using the label as index\n",
    "        class_name = class_names[label]\n",
    "\n",
    "        # Calculate the center of the bounding box\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "\n",
    "        # Draw the bounding box and label on the frame\n",
    "        color = colors[class_name]\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, f'{class_name} {conf:.2f}', (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Check if the detected class is \"customer\"\n",
    "        if class_name == 'customer':\n",
    "            # Calculate distance to the center of the screen\n",
    "            distance = np.sqrt(\n",
    "                (center_x - center_screen[0]) ** 2 + (center_y - center_screen[1]) ** 2)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_face = (x1, y1, x2, y2, conf, class_name)\n",
    "                # Update the closest center\n",
    "                closest_face_center = (center_x, center_y)\n",
    "\n",
    "    # Highlight the closest face and show center coordinates\n",
    "    if closest_face:\n",
    "        x1, y1, x2, y2, conf, class_name = closest_face\n",
    "        center_x, center_y = closest_face_center  # Retrieve the center coordinates\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), target_color, 2)\n",
    "        cv2.putText(frame, f'Target {conf:.2f}', (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, target_color, 2)\n",
    "        # Display the coordinates of the center of the target face\n",
    "        cv2.putText(frame, f'Center: ({center_x}, {center_y})', (center_x + 10, center_y + 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        # Print the center coordinates to the console\n",
    "        print(f'Target Center Coordinates: ({center_x}, {center_y})')\n",
    "\n",
    "        # Send the center coordinates every 3 seconds\n",
    "        current_time = time.time()\n",
    "        if current_time - last_sent_time >= 2:  # Check if 3 seconds have passed\n",
    "            try:\n",
    "                send_data(f\"{center_x},{center_y}\")\n",
    "                print(f\"Sent Data: {center_x},{center_y}\")\n",
    "                last_sent_time = current_time  # Update the last sent time\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "    # Display the frame with detections\n",
    "    cv2.imshow('Real-Time Detection', frame)\n",
    "\n",
    "    # Break on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tflite installer:\n",
    "pip3 install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp  # type: ignore\n",
    "import numpy as np\n",
    "# import socket\n",
    "import time  # Added for interval implementation\n",
    "import serial\n",
    "\n",
    "# init the serial for arduino\n",
    "arduino = serial.Serial(port='/dev/cu.usbserial-0001',\n",
    "                        baudrate=9600, timeout=1)\n",
    "\n",
    "# Function to send data\n",
    "\n",
    "\n",
    "def send_data(data):\n",
    "    if arduino.isOpen():\n",
    "        arduino.write(data.encode())  # Send data as bytes\n",
    "        print(f\"Sent: {data}\")\n",
    "    else:\n",
    "        print(\"Serial port is not open!\")\n",
    "\n",
    "\n",
    "# Mediapipe initializations\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "def calculate_dominance_score(face_area, eye_stability, mouth_movement):\n",
    "    \"\"\"\n",
    "    Calculate a dominance score based on normalized face area, eye stability, and mouth movement.\n",
    "    \"\"\"\n",
    "    normalized_face_area = face_area / 40000  # Adjust based on resolution\n",
    "    dominance_score = normalized_face_area * 0.1 + \\\n",
    "        eye_stability * 0.1 + mouth_movement * 0.8\n",
    "    return dominance_score\n",
    "\n",
    "\n",
    "def detect_and_track_faces():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to access the camera.\")\n",
    "        return\n",
    "\n",
    "    # Socket setup\n",
    "    server_address = ('127.0.0.1', 65432)\n",
    "    # sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "    try:\n",
    "        # sock.connect(server_address)\n",
    "        print(\"Socket connection established.\")\n",
    "    except ConnectionRefusedError:\n",
    "        print(\"Error: Unable to connect to the server.\")\n",
    "        return\n",
    "\n",
    "    with mp_face_detection.FaceDetection(min_detection_confidence=0.5) as face_detection, \\\n",
    "            mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=10, min_detection_confidence=0.5) as face_mesh:\n",
    "\n",
    "        try:\n",
    "            last_sent_time = 0  # Initialize timestamp for the interval\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Error: Unable to read the frame.\")\n",
    "                    break\n",
    "\n",
    "                # frame = cv2.flip(frame, 1)\n",
    "                height, width, _ = frame.shape\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                face_results = face_detection.process(rgb_frame)\n",
    "                dominant_face = None\n",
    "                dominant_score = -1\n",
    "                dominant_coords = (0, 0)\n",
    "\n",
    "                if face_results.detections:\n",
    "                    for detection in face_results.detections:\n",
    "                        bboxC = detection.location_data.relative_bounding_box\n",
    "                        x, y, w, h = (\n",
    "                            int(bboxC.xmin * width),\n",
    "                            int(bboxC.ymin * height),\n",
    "                            int(bboxC.width * width),\n",
    "                            int(bboxC.height * height),\n",
    "                        )\n",
    "                        face_area = w * h\n",
    "                        face_center = (x + w // 2, y + h // 2)\n",
    "\n",
    "                        # Eye and Mouth Tracking\n",
    "                        face_mesh_results = face_mesh.process(rgb_frame)\n",
    "                        eye_stability, mouth_movement = 0, 0\n",
    "\n",
    "                        if face_mesh_results.multi_face_landmarks:\n",
    "                            for landmarks in face_mesh_results.multi_face_landmarks:\n",
    "                                # Eye Tracking\n",
    "                                left_eye_indices = [\n",
    "                                    33, 160, 158, 133, 153, 144]\n",
    "                                right_eye_indices = [\n",
    "                                    362, 385, 387, 263, 373, 380]\n",
    "\n",
    "                                left_eye_coords = [(landmarks.landmark[i].x * width, landmarks.landmark[i].y * height)\n",
    "                                                   for i in left_eye_indices]\n",
    "                                right_eye_coords = [(landmarks.landmark[i].x * width, landmarks.landmark[i].y * height)\n",
    "                                                    for i in right_eye_indices]\n",
    "\n",
    "                                left_eye_stability = np.std(\n",
    "                                    [pt[1] for pt in left_eye_coords])\n",
    "                                right_eye_stability = np.std(\n",
    "                                    [pt[1] for pt in right_eye_coords])\n",
    "                                eye_stability = max(\n",
    "                                    0, 10 - (left_eye_stability + right_eye_stability) / 2)\n",
    "\n",
    "                                # Mouth Tracking\n",
    "                                mouth_indices = [78, 308, 13, 14, 87, 317]\n",
    "                                mouth_coords = [(landmarks.landmark[i].x * width, landmarks.landmark[i].y * height)\n",
    "                                                for i in mouth_indices]\n",
    "\n",
    "                                mouth_diff = np.mean(\n",
    "                                    [abs(pt[1] - mouth_coords[0][1]) for pt in mouth_coords])\n",
    "                                mouth_movement = min(15, mouth_diff)\n",
    "\n",
    "                        # Calculate dominance score\n",
    "                        dominance_score = calculate_dominance_score(\n",
    "                            face_area, eye_stability, mouth_movement)\n",
    "\n",
    "                        # Update dominant face\n",
    "                        if dominance_score > dominant_score + 0.6:\n",
    "                            dominant_score = dominance_score\n",
    "                            dominant_face = face_center\n",
    "                            dominant_coords = (x, y, w, h)\n",
    "\n",
    "                    # Send dominant face coordinates via socket at 1-second intervals\n",
    "                    current_time = time.time()\n",
    "                    if dominant_face and current_time - last_sent_time >= 2:\n",
    "\n",
    "                        message = f\"{dominant_face[0]},{dominant_face[1]}\"\n",
    "                        # sock.sendall(message.encode('utf-8'))\n",
    "                        last_sent_time = current_time  # Update last sent time\n",
    "                        send_data(message)\n",
    "\n",
    "                        # Highlight dominant face\n",
    "                        x, y, w, h = dominant_coords\n",
    "                        cv2.rectangle(\n",
    "                            frame, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
    "                        cv2.putText(frame, f\"Dominant Face - Coords: {dominant_face}, Score: {dominant_score:.2f}\",\n",
    "                                    (x, y - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "                        # Print coordinates and score to the terminal\n",
    "                        print(\n",
    "                            f\"Dominant Face Coords: {dominant_face}, Dominance Score: {dominant_score:.2f}\")\n",
    "\n",
    "                # Display the frame\n",
    "                cv2.imshow(\"Dominant Face Tracking\", frame)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "        finally:\n",
    "            # sock.close()\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect_and_track_faces()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
